import re
from datetime import datetime, timedelta
from typing import List, Dict, Any
from utils.config import Config

class NewsFilter:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π"""
    
    def __init__(self):
        self.config = Config()
    
    def is_recent_news(self, published_date: datetime, hours: int = 24) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –Ω–æ–≤–æ—Å—Ç—å —Å–≤–µ–∂–µ–π (–∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —á–∞—Å–æ–≤)"""
        if not published_date:
            return False
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è —Å —É—á–µ—Ç–æ–º —á–∞—Å–æ–≤–æ–≥–æ –ø–æ—è—Å–∞
        from datetime import timezone
        now = datetime.now(timezone.utc)
        cutoff_time = now - timedelta(hours=hours)
        
        # –ï—Å–ª–∏ published_date –Ω–µ –∏–º–µ–µ—Ç —á–∞—Å–æ–≤–æ–≥–æ –ø–æ—è—Å–∞, –¥–æ–±–∞–≤–ª—è–µ–º UTC
        if published_date.tzinfo is None:
            published_date = published_date.replace(tzinfo=timezone.utc)
        
        return published_date >= cutoff_time
    
    def contains_ai_keywords(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –æ–± –ò–ò"""
        if not text:
            return False
        
        text_lower = text.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        has_ai_keywords = any(keyword.lower() in text_lower for keyword in self.config.AI_KEYWORDS)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∏—Å–∫–ª—é—á–∞–µ–º—ã—Ö —Å–ª–æ–≤
        has_exclude_keywords = any(keyword.lower() in text_lower for keyword in self.config.EXCLUDE_KEYWORDS)
        
        return has_ai_keywords and not has_exclude_keywords
    
    def is_retweet(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–≤–∏—Ç —Ä–µ—Ç–≤–∏—Ç–æ–º"""
        return text.startswith('RT @') or text.startswith('rt @')
    
    def clean_text(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        if not text:
            return ""
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        return text
    
    def extract_keywords_from_text(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return []
        
        text_lower = text.lower()
        found_keywords = []
        
        for keyword in self.config.AI_KEYWORDS:
            if keyword.lower() in text_lower:
                found_keywords.append(keyword)
        
        return found_keywords
    
    def is_english_or_russian(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏–ª–∏ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ"""
        if not text:
            return False
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ —Ä–∞–∑–Ω—ã—Ö –∞–ª—Ñ–∞–≤–∏—Ç–æ–≤
        cyrillic_count = len(re.findall(r'[–∞-—è—ë]', text.lower()))
        latin_count = len(re.findall(r'[a-z]', text.lower()))
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã (—è–ø–æ–Ω—Å–∫–∏–µ, –∫–∏—Ç–∞–π—Å–∫–∏–µ, –∞—Ä–∞–±—Å–∫–∏–µ –∏ —Ç.–¥.)
        unwanted_chars = len(re.findall(r'[^\w\s\-.,!?()\[\]":;@#$%^&*+=<>/\\|`~]', text))
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–Ω–∞—á–∏–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        total_letters = cyrillic_count + latin_count
        total_chars = len(re.findall(r'[^\s]', text))  # –í—Å–µ –Ω–µ-–ø—Ä–æ–±–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        
        if total_letters == 0:
            return False
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã - –∏—Å–∫–ª—é—á–∞–µ–º
        if unwanted_chars > 0:
            return False
        
        # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 80% —Å–∏–º–≤–æ–ª–æ–≤ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã –∏–ª–∏ –ª–∞—Ç–∏–Ω–∏—Ü—ã - —Å—á–∏—Ç–∞–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–º
        cyrillic_ratio = cyrillic_count / total_letters
        latin_ratio = latin_count / total_letters
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –º–∏–Ω–∏–º—É–º 3 –±—É–∫–≤—ã –∏ 80% –∏–∑ –Ω–∏—Ö - –∫–∏—Ä–∏–ª–ª–∏—Ü–∞ –∏–ª–∏ –ª–∞—Ç–∏–Ω–∏—Ü–∞
        return total_letters >= 3 and (cyrillic_ratio > 0.8 or latin_ratio > 0.8)
    
    def calculate_relevance_score(self, title: str, description: str = "", keywords: List[str] = None) -> int:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ (0-100)"""
        if not title:
            return 0
        
        score = 0
        text = f"{title} {description}".lower()
        
        # –ë–∞–∑–æ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        high_priority = ["chatgpt", "openai", "claude", "gemini", "sora", "gpt-4", "gpt-5"]
        for keyword in high_priority:
            if keyword in text:
                score += 20
        
        # –°—Ä–µ–¥–Ω–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        medium_priority = ["ai", "artificial intelligence", "stable diffusion", "midjourney"]
        for keyword in medium_priority:
            if keyword in text:
                score += 10
        
        # –ù–∏–∑–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        low_priority = ["machine learning", "deep learning", "neural network"]
        for keyword in low_priority:
            if keyword in text:
                score += 5
        
        # –ë–æ–Ω—É—Å –∑–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        if keywords:
            score += min(len(keywords) * 5, 20)
        
        return min(score, 100)
    
    def filter_news_by_relevance(self, news_list: List[Dict[str, Any]], min_score: int = 30) -> List[Dict[str, Any]]:
        """–§–∏–ª—å—Ç—Ä—É–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ —è–∑—ã–∫—É"""
        filtered_news = []
        
        for news in news_list:
            title = news.get('title', '')
            description = news.get('description', '')
            keywords = news.get('keywords', [])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —è–∑—ã–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            if not self.is_english_or_russian(title):
                continue
            
            score = self.calculate_relevance_score(title, description, keywords)
            
            if score >= min_score:
                news['relevance_score'] = score
                filtered_news.append(news)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        filtered_news.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
        
        return filtered_news
    
    def remove_duplicates(self, news_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –Ω–æ–≤–æ—Å—Ç–µ–π"""
        seen_titles = set()
        seen_urls = set()
        unique_news = []
        
        for news in news_list:
            title = news.get('title', '').lower().strip()
            url = news.get('url', '')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É –∏ URL
            if title not in seen_titles and url not in seen_urls:
                seen_titles.add(title)
                seen_urls.add(url)
                unique_news.append(news)
        
        return unique_news
    
    def format_news_for_telegram(self, news_list: List[Dict[str, Any]], max_items: int = 20) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram"""
        if not news_list:
            return "üì∞ *AI News Digest*\n\n–ù–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞."
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π
        news_list = news_list[:max_items]
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É
        from datetime import timezone
        current_date = datetime.now(timezone.utc).strftime("%d %B %Y")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        header = f"üì∞ <b>AI News Digest ‚Äî {current_date}</b>\n\n"
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
        news_items = []
        for i, news in enumerate(news_list, 1):
            title = news.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')
            url = news.get('url', '#')
            source = news.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫')
            content_type = news.get('content_type', '')
            duration = news.get('duration', 0)
            
            # –ù–µ –æ–±—Ä–µ–∑–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–ª–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
            
            # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–æ–¥–∑–∏ –¥–ª—è —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ Reddit
            content_emoji = ""
            if 'Reddit' in source and content_type:
                emoji_map = {
                    'image': 'üñºÔ∏è',
                    'video': 'üé•', 
                    'text': 'üìù',
                    'link': 'üîó'
                }
                content_emoji = emoji_map.get(content_type, 'üìÑ')
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è YouTube –≤–∏–¥–µ–æ
            duration_info = ""
            if 'YouTube' in source and duration > 0:
                minutes = duration // 60
                seconds = duration % 60
                if minutes > 0:
                    duration_info = f" ({minutes}–º {seconds}—Å)"
                else:
                    duration_info = f" ({seconds}—Å)"
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É —Å —É—á–µ—Ç–æ–º —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            if content_emoji:
                news_item = f"üîπ {content_emoji} <a href='{url}'>{title}</a>{duration_info}\n–ò—Å—Ç–æ—á–Ω–∏–∫: {source}"
            else:
                news_item = f"üîπ <a href='{url}'>{title}</a>{duration_info}\n–ò—Å—Ç–æ—á–Ω–∏–∫: {source}"
            
            news_items.append(news_item)
        
        return header + "\n".join(news_items)
